---
title: "Final Project"
author: "Colleen Burke"
date: "2025-12-02"
output: word_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(tidyverse)
library(readr)
library(purrr)
library(sandwich)
library(knitr)
library(broom)
library(here)
library(ggplot2)
library(viridis)

set.seed(12345)
```

In this final project, a Monte Carlo simulation was used to compare Poisson and Normal regression models for count outcomes. The goal is to examine how model misspecification, sample size, and the true rate parameter (λ) influence bias in λ̂ and the variability of β̂₁ under naïve and robust variance estimators.

# Background

Generalized linear models (GLMs) extend ordinary regression to non-normal outcomes.  
This project evaluates how model **misspecification** (fitting a Normal model to Poisson data) affects:

- Bias of λ̂  
- Variance of β̂  
- Naïve vs robust variance estimates  
- Differences by sample size and true λ  

We simulate Poisson outcomes using fixed binary predictors and compare Normal vs Poisson regression.


# Methods

## Load x-vectors

```{r load-x, echo = FALSE}
x20  <- read_csv(here("x_n20-1.csv")) |> pull(x)
x200 <- read_csv(here("x_n200.csv"))   |> pull(x)

length(x20)
length(x200)
```



## Simulation Functions

```{r sim-functions, echo = FALSE}

simulate_data <- function(N, lambda, x_vector) {
  x_vector <- as.numeric(x_vector)
  N_x <- length(x_vector)
  
  if (N_x != N) {
    stop("Length mismatch: x length = ", N_x, " but N = ", N)
  }
  
  tibble(
    x = x_vector,
    y = rpois(N_x, lambda)
  )
}

run_one_sim <- function(N, lambda, x_vector) {
  
  dat <- simulate_data(N, lambda, x_vector)
  
  fit_pois <- glm(y ~ x, family = poisson, data = dat)
  fit_norm <- glm(y ~ x, family = gaussian, data = dat)
  
  # create a data frame so we can predict fitted mean counts at each observed x
  new_data <- data.frame(x = x_vector)
  
  pois_lambda_hat <- predict(fit_pois, newdata = new_data, type = "response")
  norm_lambda_hat <- predict(fit_norm, newdata = new_data, type = "response")
  
  # compute average bias of λ̂ across all observations in this simulated dataset
  pois_bias <- mean(pois_lambda_hat - lambda)
  norm_bias <- mean(norm_lambda_hat - lambda)
  
  # extract the variance of β̂₁ (2nd coefficient) from each model’s variance–covariance matrix
  naive_pois  <- vcov(fit_pois)[2, 2]
  robust_pois <- sandwich(fit_pois)[2, 2]
  
  naive_norm  <- vcov(fit_norm)[2, 2]
  robust_norm <- sandwich(fit_norm)[2, 2]
  
  tibble(
    N = N,
    lambda = lambda,
    beta1_pois  = coef(fit_pois)[2],
    beta1_norm  = coef(fit_norm)[2],
    pois_bias   = pois_bias,
    norm_bias   = norm_bias,
    naive_pois  = naive_pois,
    robust_pois = robust_pois,
    naive_norm  = naive_norm,
    robust_norm = robust_norm
  )
}

```

## Run Simulations

```{r run-sims, echo = FALSE}
Ns <- c(20, 200)
lambdas <- c(4, 10)
B <- 2000

results_list <- list()

for (N in Ns) {
  
  x_vec <- if (N == 20) x20 else x200
  
  for (lambda in lambdas) {
    res <- map_dfr(1:B, ~ run_one_sim(N, lambda, x_vec))
    results_list[[paste0("N", N, "_lambda", lambda)]] <- res
  }
}

all_results <- bind_rows(results_list, .id = "scenario")
```



# Results

## Bias of λ̂

```{r bias-table, echo = FALSE}
bias_table <- all_results |>
  group_by(N, lambda) |>
  summarise(
    mean_pois_bias = mean(pois_bias),
    mean_norm_bias = mean(norm_bias),
    .groups = "drop"
  )

knitr::kable(bias_table, digits = 4,
             caption = "Average Bias of λ̂ Across Scenarios")
```
Across all combinations of N and λ, the average bias is very close to zero for both models, indicating that λ̂ is essentially unbiased in this setting.


## Variances of β̂ (Naïve vs Robust)

```{r var-table, echo = FALSE}
var_table <- all_results |>
  group_by(N, lambda) |>
  summarise(
    naive_norm  = mean(naive_norm),
    robust_norm = mean(robust_norm),
    naive_pois  = mean(naive_pois),
    robust_pois = mean(robust_pois),
    .groups = "drop"
  )

knitr::kable(var_table, digits = 4,
             caption = "Naïve and Robust Variances of β̂₁")
```

The Poisson model shows similar naïve and robust variances, while the Normal model tends to have larger robust than naïve variances, especially in smaller samples, suggesting that naive variances under misspecification can underestimate true uncertainty.

## Confidence Intervals for β̂₁

```{r ci-data, echo = FALSE}
ci_summary <- all_results |>
  group_by(N, lambda) |>
  summarise(
    beta_mean_pois  = mean(beta1_pois),
    se_naive_pois   = sqrt(mean(naive_pois)),
    se_robust_pois  = sqrt(mean(robust_pois)),
    beta_mean_norm  = mean(beta1_norm),
    se_naive_norm   = sqrt(mean(naive_norm)),
    se_robust_norm  = sqrt(mean(robust_norm)),
    .groups = "drop"
  )

ci_summary <- bind_rows(
  ci_summary |>
    transmute(
      N, lambda,
      model   = "Poisson",
      var_type = "Naive",
      beta    = beta_mean_pois,
      se      = se_naive_pois
    ),
  ci_summary |>
    transmute(
      N, lambda,
      model   = "Poisson",
      var_type = "Robust",
      beta    = beta_mean_pois,
      se      = se_robust_pois
    ),
  ci_summary |>
    transmute(
      N, lambda,
      model   = "Normal",
      var_type = "Naive",
      beta    = beta_mean_norm,
      se      = se_naive_norm
    ),
  ci_summary |>
    transmute(
      N, lambda,
      model   = "Normal",
      var_type = "Robust",
      beta    = beta_mean_norm,
      se      = se_robust_norm
    )
) |>
  mutate(
    lower = beta - 1.96 * se,
    upper = beta + 1.96 * se
  )
```


```{r ci-plot, fig.width=8, fig.height=6, echo = FALSE}
ggplot(ci_summary, 
       aes(x = model, y = beta, color = var_type)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(
    aes(ymin = lower, ymax = upper),
    width = 0.15,
    position = position_dodge(width = 0.5),
    linewidth = 1
  ) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray30") +
  scale_color_viridis_d(option = "plasma", end = 0.8, name = "Variance Type") +
  facet_grid(lambda ~ N, labeller = labeller(
    lambda = function(x) paste0("λ = ", x),
    N = function(x) paste0("N = ", x)
  )) +
  labs(
    title = "95% Confidence Intervals for β₁",
    x = "Model Type",
    y = expression(hat(beta)[1])
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 18),
    strip.text = element_text(size = 12, face = "bold"),
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

```
These confidence intervals show that both models produce β̂₁ estimates centered near zero, but robust intervals, especially for the misspecified Normal model at N = 20—are wider, better capturing sampling variability while all intervals tighten as N and λ increase.


# Conclusions

Across all simulation scenarios, model misspecification affected uncertainty more than point estimates. Both the Poisson and Normal models produced mean estimates of λ with minimal bias, indicating that misspecifying the outcome distribution did not materially distort the mean structure in this simple setup.

Variance estimates, however, were more sensitive to misspecification. For correctly specified Poisson models, naïve and robust variances were nearly identical. In contrast, the Normal model applied to Poisson data showed consistently larger robust variances, especially in smaller samples, which highlights that naive variances from a misspecified model can underestimate true sampling variability.

Sample size had a predictable stabilizing effect: increasing N from 20 to 200 reduced variability in estimates and diminished differences between models and variance estimators. Larger samples helped the Normal model behave more similarly to the Poisson model.

Finally, increasing the true rate from λ=4 to λ=10 increased variability, as expected from the Poisson mean–variance relationship. Because the distribution becomes more symmetric at higher λ, the consequences of misspecification were slightly less noticeable at λ=10.

Overall, misspecification had little effect on bias but clear effects on variance (particularly when sample sizes were small or λ was low) which underscores when correct distributional assumptions and robust variance estimators matter most.



